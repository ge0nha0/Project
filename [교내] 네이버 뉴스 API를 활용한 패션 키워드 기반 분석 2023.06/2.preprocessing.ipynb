{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70518d3",
   "metadata": {},
   "source": [
    "# 2. 텍스트 데이터 전처리\n",
    "## 2.1 Cleaning\n",
    "### 제목과 내용을 기준으로 중복 기사 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복기사 제거\n",
    "# 데이터 Concat 후 중복값(제목, 내용 기준으로) 제거 # 12,025 -> 10,526\n",
    "\n",
    "df = pd.concat([fs_10,fs_20, fs_30, fs_40, fs_50, fs_1980, fs_1990, fs_2000, fs_2010, fs_2020,\n",
    "           fs_m, fs_w, fs_sp, fs_su, fs_au, fs_wi, fs_usa, fs_jap, fs_fra, fs_ita, fs_eng,\n",
    "           fs_spa, fs_nike, fs_lux, fs_work], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목과 내용 기준으로 첫번째 겹친 것은 살려두고 나머지는 제거\n",
    "df = df.drop_duplicates(['제목', '내용'], keep = 'first', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5856bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식을 통한 데이터 전처리\n",
    "import re\n",
    "\n",
    "# '\\n사진이미지사진자료기자기사뉴스' 패턴을 포함하는 문자열을 공백으로 대체하여 제거\n",
    "regex = r'[\\n사진이미지사진자료기자기사뉴스]'\n",
    "df['내용'] = df['내용'].apply(lambda x: re.sub(regex, '', x))\n",
    "\n",
    "df['전처리 내용'] = df['내용'].copy()\n",
    "\n",
    " # '\\w' (알파벳과 숫자), '\\s' (공백)이 아닌 문자를 공백으로 대체하여 제거\n",
    "regex = r'[^\\w\\s]' \n",
    "\n",
    "df['전처리 내용'] = df['내용'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "# '\\n', '\\t', '\\r'을 빈 문자열로 대체하여 제거\n",
    "df['전처리 내용'] = df['전처리 내용'].str.replace('\\n', '')\n",
    "df['전처리 내용'] = df['전처리 내용'].str.replace('\\t', '')\n",
    "df['전처리 내용'] = df['전처리 내용'].str.replace('\\r', '')\n",
    "\n",
    "# 문자열 앞뒤의 공백을 제거\n",
    "df['전처리 내용'] = df['전처리 내용'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc239b1",
   "metadata": {},
   "source": [
    "# Tokenization & Pos Tagging & Stopword Removal\n",
    "stopwords들을 정의하고 이를 제외해서 분석하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea74997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt         \n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06578028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "li1 = [] # 토큰화된 결과를 저장할 리스트\n",
    "# df 데이터프레임의 각 행에 대해 반복\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    # 현재 행의 content 값을 df_text 변수에 할당\n",
    "    df_text = row['전처리 내용']  \n",
    "    # df_text를 한국어 형태소 분석기를 사용하여 토큰화한 결과를 ko 변수에 할당  \n",
    "    ko = t.pos(df_text)\n",
    "    li1.append(ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일곱번째 행 ->  Pos Tagging\n",
    "print(li1[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 별 구체적으로 토큰화\n",
    "li2 = [] # 단어별로 구체적으로 토큰화된 결과를 저장할 리스트\n",
    "for li in li1: \n",
    "    li3 = [] # 단어별로 구체적으로 토큰화된 결과를 저장할 리스트\n",
    "    for ele in li: \n",
    "        li3.append(ele[0]) # 토큰화된 결과의 첫 번째 요소(단어)를 리스트에 추가\n",
    "    li2.append(li3)  # 단어별로 구체적으로 토큰화된 결과를 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a856d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일곱번째 행 -> 구체적 토큰화 진행\n",
    "print(li2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58090bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword Removal\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Naver_뉴스_패션/data/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.replace('\\n','') for x in stopwords]\n",
    "okt = Okt()\n",
    "\n",
    "toks = li2 # 토큰화된 결과 리스트\n",
    "li4 = [] # 불용어 제거된 결과를 저장할 리스트\n",
    "for tok in toks:\n",
    "    li5 = []  # 불용어가 아닌 토큰을 저장할 리스트\n",
    "    for to in tok:\n",
    "         if to not in stopwords: # 토큰이 불용어가 아닌 경우에만 처리\n",
    "            li5.append(to)\n",
    "    li4.append(li5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일곱번째 행 -> Stopword 제거\n",
    "print(li4[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe 생성\n",
    "df_li = []\n",
    "for tok in li4:\n",
    "    to = ' '.join(tok)\n",
    "    df_li.append(to) \n",
    "\n",
    "df1 = pd.DataFrame(df_li).rename(columns = {0:'전처리 내용2'})\n",
    "df2 = pd.concat([df,df1],axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6afdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('/content/drive/MyDrive/Colab Notebooks/Naver_뉴스_패션/data/df2.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Naver_뉴스_패션/data/df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 내용2에서 결측값 2개 발견\n",
    "df2['전처리 내용2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21100150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 내용2 결측치 제거\n",
    "df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거 결과 0\n",
    "df2['전처리 내용2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f93832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(['제목', '내용', '전처리 내용'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0d40d",
   "metadata": {},
   "source": [
    "# 키워드 열을 기준으로 다시 데이터프레임을 나누기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8953ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_10 = df3[df3['키워드'] == '10대패션']\n",
    "fs_20 = df3[df3['키워드'] == '20대패션']\n",
    "fs_30 = df3[df3['키워드'] == '30대패션']\n",
    "fs_40 = df3[df3['키워드'] == '40대패션']\n",
    "fs_50 = df3[df3['키워드'] == '50대패션']\n",
    "\n",
    "fs_1980 = df3[df3['키워드'] == '1980년대패션']\n",
    "fs_1990 = df3[df3['키워드'] == '1990년대패션']\n",
    "fs_2000 = df3[df3['키워드'] == '2000년대패션']\n",
    "fs_2010 = df3[df3['키워드'] == '2010년대패션']\n",
    "fs_20200 = df3[df3['키워드'] == '2020년대패션']\n",
    "\n",
    "fs_m = df3[df3['키워드'] == '남자패션']\n",
    "fs_w = df3[df3['키워드'] == '여자패션']\n",
    "\n",
    "fs_sp = df3[df3['키워드'] == '봄패션']\n",
    "fs_su = df3[df3['키워드'] == '여름패션']\n",
    "fs_au = df3[df3['키워드'] == '가을패션']\n",
    "fs_wi = df3[df3['키워드'] == '겨울패션']\n",
    "\n",
    "fs_usa = df3[df3['키워드'] == '미국패션']\n",
    "fs_jap = df3[df3['키워드'] == '일본패션']\n",
    "fs_fra = df3[df3['키워드'] == '프랑스패션']\n",
    "fs_ita = df3[df3['키워드'] == '이탈리아패션']\n",
    "fs_eng = df3[df3['키워드'] == '영국패션']\n",
    "\n",
    "fs_spa = df3[df3['키워드'] == 'SPA']\n",
    "fs_nike = df3[df3['키워드'] == '나이키']\n",
    "fs_lux = df3[df3['키워드'] == '명품']\n",
    "fs_work = df3[df3['키워드'] == '패션워크']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
